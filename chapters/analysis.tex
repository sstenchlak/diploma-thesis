\chapter{Requirement analysis}\label{chapters:analysis}

This chapter summarizes expectations for the application in a form of requirements. The requirements are analyzed and the solution is proposed in the next chapter with focus on formal description of the framework.

\bigskip

As a \textbf{stakeholder} we would consider data analyst, programmers, or at least people interested in the area of data-modelling, as the typical use-case of the application is (i) to design schemas for a large system of interconnected subystems or modules, or (ii) to design a recommendation for publishing data. Both these use-cases were described in the introduction of this chapter.

Because of the knowledge in the area of data modelling of all the stakeholders, we may have the UI of the application for those users more technical as the intent of the application and all the operations may be intuitive for them.

Nevertheless, the basic functionality does not require the advanced knowledge in the fields mentioned above, therefore we propose an "expert mode". User will be asked whether he or she feels to be an expert in the field which would make available more advanced features of the application, while keeping the UI simple for those interested in basics of data modelling.

\bigskip

\begin{requirement}
A user shall be able to easily derive a \textbf{general schema} structure from the existing ontologies and then translate the structure into different known schema languages, such as JSON Schema, XSD and CSVW Schema and it shall be possible to add support for others easily.
\end{requirement}

The basic idea behind this requirement was already explained in the introduction chapter. From an ontology, which specifies the relations between things from a real world, it should be possible to easily select relations and things that will describe a schema. The schema then describes a structure of data that represents those things.

\smallskip

Our goal is to design a model capable of describing a data schema of common data formats. This schema will be used as a mapping from the ontology to the desired structure. The structure model must be robust enough to support different formats, as we want to use the same model for different formats.

Data serialization is a process of storing object state in a format that can be read back later. There are many different models for serialization with the most known as:
\begin{itemize}
    \item \textbf{Hiearchical model} stores data in a tree-like structure. This was one of the most common model for serialization of data in past few decades as it was easy to understand and interpret. XML and JSON are examples of formats that use this model.
    \item \textbf{Relational model} uses tables to store data. Each column is defined in advance and rows denotes the entities. Rows may point to rows in other tables to link data.
    \item \textbf{Graph model} represents data in graph.
\end{itemize}

% Tady vlastne rozeberu, ze ok, je to ve stromove strukture. No a ted jak by mela vypadat a proc zrovna takhle. Zamerim se na asociaci na tridu, ktera je na jedom radku. Nejaky obecny kocept, jak se treba vydat i do budoucna a jake jsou alternativey, napriklad ve srovnani s xcase, kde to bylo stromove.

As our main intent is to support JSON and XML, we will use the first type of model to represent data in our general format. The translation from that format to individual schemas in hiearchical model would be implicit.

Supporting translation from the general schema in hiearchical model to other formats in relational and graph model should be possible in a limited way\footnote{That means we may not be able to reverse translation from specific schema to the general schema or it may not be possible to use the full power of the given specific schema. However, this is not important to us as our target is support for basic use-cases.}, which is sufficient and is in accordance with the requirement to have one general schema. The last type of model is not even necessary to generate as we use the ontology that is already in graph model, hence we can use directly the ontology as the schema to validate our data.

To make the model applicable in practice, we need to support advanced format-specific constructs as well. Those constructs should not forbid us to use the model for other formats if possible and should be easily adaptable to new formats.

Because of a vast majority of different schema languages, we cannot have too strict requirements on the model or the ontology, as some schemas may not require all the information or may be too simple. This pushes us to define these two concepts in the most elementary way.

\begin{figure}\centering
    \begin{tikzpicture}[
        squarednode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
        schema/.style={rectangle, draw=yellow!60, fill=yellow!5, very thick, minimum size=5mm},
        ontology/.style={rectangle, draw=purple!60, fill=purple!5, very thick, minimum size=5mm},
    ]
        %Nodes
        \node[ontology] (ontology) at (0,0) {Ontology};

        \node[squarednode] (schema1) at (-3.5,-1.5) {Universal schema 1};
        \node (psmDot) at (0,-1.5) {...};
        \node[squarednode] (schemaN) at (3.5,-1.5) {Universal schema N};

        \node[schema,align=center] (xml1) at (-5.5,-3) {XML\\schema};
        \node[schema,align=center] (json1) at (-3.5,-3) {JSON\\schema};
        \node[schema,align=center] (csv1) at (-1.5,-3) {CSV\\schema};

        \node (psmDot) at (0,-3) {...};

        \node[schema,align=center] (xmlN) at (1.5,-3) {XML\\schema};
        \node[schema,align=center] (jsonN) at (3.5,-3) {JSON\\schema};
        \node[schema,align=center] (csvN) at (5.5,-3) {CSV\\schema};

        %Lines
        \draw[-latex] (ontology) -- (schema1);
        \draw[-latex] (ontology) -- (schemaN);

        \draw[-latex] (schema1) -- (xml1);
        \draw[-latex] (schema1) -- (json1);
        \draw[-latex] (schema1) -- (csv1);

        \draw[-latex] (schemaN) -- (xmlN);
        \draw[-latex] (schemaN) -- (jsonN);
        \draw[-latex] (schemaN) -- (csvN);
    \end{tikzpicture}
    \caption{Diagram showing the core workflow behind the data modelling from a ontology. User can create universal schemas (blue rectangles) from the ontology from which are created traditional data schemas, such as XSD, CSV Schema or JSON schema.}
\end{figure}

%Tady pozadavek na tu strukturu, tedy ze bude stromova, do radku chceme sjednocovat asociace a celkove to ma byt barevne a ikonkove odlisene.



We have already stated that a hiearchical model is preferred for the internal universal schema as it can be used to create also CSVs, that are in a relational model. Although we havent specified how the structure should look like, there might be references, as the schema may point to other schemas. This

Previous tools \textit{xCase} and \textit{eXolutio} used full graph editor instead of simplier list to show and modify the universal schema. In the general case, users may benefit from the graph view if the schema refers to another schema multiple times, because this can be easily denoted in the graphical interface (see \autoref{analysis/difference-between-graphical-and-hiearchical}). On the other hand, schemas such as XML or CSV often depends on the order of its elements. This is easier to capture in the hiearchical model as all the items are in the list which has some order implicitly.

\begin{figure}\centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \begin{tikzpicture}[
            squarednode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
            schema/.style={rectangle, draw=yellow!60, fill=yellow!5, very thick, minimum size=5mm},
            ontology/.style={rectangle, draw=purple!60, fill=purple!5, very thick, minimum size=5mm},
        ]
            %Nodes
            \node[ontology] (root) at (0,0) {root class};

            \node[squarednode] (a1) at (-1.5,-1.5) {association 1};
            \node[squarednode] (a2) at (1.5,-1.5) {association 2};

            \node[ontology] (ref) at (0,-3) {referenced class};

            %Lines
            \draw[-latex] (root) -- (a1);
            \draw[-latex] (root) -- (a2);
            \draw[-latex] (a1) -- (ref);
            \draw[-latex] (a2) -- (ref);
        \end{tikzpicture}
        \caption{Graphical representation}
      \end{subfigure}%
      \begin{subfigure}{.5\textwidth}
\begin{Verbatim}[commandchars=\\\{\}]
{\color{purple!60}root class}
  {\color{blue!60}- association 1}
    {\color{purple!60}- referenced class}
  {\color{blue!60}- association 2}
    {\color{purple!60}- referenced class}
\end{Verbatim}
        \caption{Hiearchical representation}
      \end{subfigure}

    \caption{Figure showing a schema referencing the same subschema twice, essencially creating a cycle in unoriented graph. Two different representations are shown - graph and hiearchical.  The former one shows that both associations refer the same subschema, which later representation can not show.}
    \label{analysis/difference-between-graphical-and-hiearchical}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{requirement}
    The application shall create a supporting documents to the generated schemas.
\end{requirement}

As the main goal of the application should be to generate schemas, we also need to create documentations to them, diagrams and examples. This requirement has already been described in the introduction chapter. % generovani obrazku ontologie: popsat proc to chceme a jak to ma fungovat, to uz by melo byt v introduction

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{requirement}
    List of supported schemas, transformations, documents, and other files generated from the universal schema shall be easily expandable so that the application can be adapted to different use-cases.
\end{requirement}

Generation of schemas is robust enough to be used in every common scenario, therefore we do not expect that user may want to intervene the process besides the standart configuration, such as indentation, using of comments, or a default language.

On the other hand, documentation is very vague concept that neither we have properly specified. Sometimes a simple Markdown documentation may be sufficient, while elsewhere user may require a strict format of multiple documents in HTML.

Transformations have similar issue. There are multiple ways and technologies that transform data between different schemas. We have already mentioned transformation throught RDF format, either by RML, or custom scripts, such as XSLT for XML. For more demanding user, it is even possible to create transformation scripts between pairs of technology, such as between XML and CSV.

We will expose a way user can register his/her own generator that can create a set of files in a filesystem from the given schema. Generators may call others to modify its results or further expends it.
% Pozadavek na OFN?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{requirement}
    \label{requirement:ontologies-on-the-web}
    As many ontologies are located on the web in formats like OWL (Web Ontology Language), RDFs (RDF Schema), UFO (Unified Foundational Ontology), etc., the application shall support reading them.
\end{requirement}

Although it may be easier for a data modeller to design the whole ontology directly in the application, where it will be later used (this approach was used in tools \textit{xCase} and \textit{eXolutio}), it may not be a good solution from several reasons:

\begin{enumerate}
    \item Designing an ontology is well-defined problem for which exists great and time-proven tools which we could not cope with.
    \item Even if the ontology will be used just to generate the schemas, it may be worthy to publish it anyways as others may benefit from it.
    \item It is better to split a complex problem to smaller ones.
\end{enumerate}

On the other hand, not having a direct access to the ontology, as it will be on the web may have following impacts:

\begin{enumerate}
    \item The ontology may \textbf{not always be available}. Unavailability should not forbid us from generating the schemas and making minor changes to them if those changes are not directly related to exploring the ontology.
    \item The concepts in the ontology may \textbf{point to another ontology} according to Linked Open Data principles.
\end{enumerate}

From the reasons above, it will be preffered to use the later approach - an ontology on the web. Later in the text, there is \autoref{requirement:pim-editing} specifying, that user shall modify the ontology directly in the application. This is not inconsistent with the statements above as it deals with small changes, rather defining a complete ontology.

The term ontology has already been defined in the introductory chapter and we will formally define the specific requirements on it in the next chapter. It shall be easy to add support for other types of ontologies and all of them shall be linkable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{requirement}
    \label{requirement:pim-editing}
    The approach from previous tools of creating the ontology directly in the application is not required, but there should be support for \textit{some} modifications.
\end{requirement}

As stated in the \autoref{requirement:ontologies-on-the-web}, the preffered way is to create a complete ontology externally and keep it up-to-date and valid against requirements of all involved parties. But it is hard to always follow all the rules, especially when doing experiments, or small errors need to be fixed.

Allowing such changes must be made carefully, as it may interfere with some mechanisms.
\begin{enumerate}
    \item If the ontology changes, the local overwrites may need to be changed as well, otherwise may become invalid. Overwritten data may get removed or moved elsewhere. Evolution mechanism hence must work with the overwrites as well.
    \item Moreover, the overwriten data may change, which can lead to two scenarios. Either user wishes to keep the local version as if nothing happend, or he/she may want to discard the local version as the new version fixes the issue that caused the modification in the first place.
\end{enumerate}

This issue is too complex and due to the nature of the requirement, it must be solved directly in the application. We keep the question behind this problem partially open and focus only to simple modifications, as this will cover most use-cases.\footnote{From our specific use-case on the Semantic government vocabulary (SGOV), most of the changes consist of adding a missing cardinality or fixing labels and descriptions.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{requirement}
    The application shall support generating transformations between different data formats conforming supported schemas and RDF representation.
\end{requirement}

Data transformation were also introduced at the beginning of this thesis. Generally speaking, \textbf{data transformations} are used to convert data (not schemas, but data that conforms given schemas) from one schema to another without changing its meaning.

One example may be to convert CSV to JSON array of object, where each object represent a row in the CSV. There are plenty of online tools to do it, but they do not understand the context of the data. On the other hand, because both schemas were designed in the tool, we may exploit the known mapping to the original ontology and properly map columns from CSV to the fields in JSON object.

In the context of this tool, tranformation means both (i) transfomation between different schemas under the same universal schema and (ii) between different universal schemas if it is possible.

There are plenty of ways how to transform data.
\begin{enumerate}
    \item Data engineers use \textbf{Python} with support of many formats using libraries. In this case, transformation would mean a generated Python script with pre-defined interface, that takes data from one format and outputs in another.
    \item There is \textbf{XSLT} (Extensible Stylesheet Language Transformations) language for transforming between XML documents or from XML to XML-like, plain-text or CSV documents. XSLT is a XML document that can be executed with input document by XSLT processor producing an resulting document. One disadvantage is that the input document must be in XML format, hence it can not be used alone for JSON and CSV transformation.
    \item There are mapping tools such as RML (RDF Mapping Language) specifically designed for mapping purposes. RML maps common serialization frameworks such as XML, CSV, and JSON to RDF from a set of rules written in RDF. The translation mechanism is similar to the XSLT.
\end{enumerate}

Although RML is a ready-to-use solution with support for all three technologies, it requires its own toolchain for transformation. In contrast, XSLT is well-known technology among people working with XML and is widely supported. Therefore, our primary concern will be to implement XSLT for XML while keeping the RML for latex.

Similar to translational models, there are two approaches. Either create transformation for each pair or have one standard format where all others can be transformed and vice versa. The latter approach is more straightforward regarding different transformations needed and requires only one transformation for each new format added. Because the schemas are built from ontologies whose primary source is RDF, we will exploit this and have RDF as the middle format, which creates symmetry and gives us another format we can transform data.

We categorize two types of transformations - lifting and lowering.

\begin{figure}\centering
    \begin{tikzpicture}[
        squarednode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm,anchor=north},
        schema/.style={rectangle, draw=yellow!60, fill=yellow!5, very thick, minimum size=5mm,anchor=north},
        ontology/.style={rectangle, draw=purple!60, fill=purple!5, very thick, minimum size=5mm,anchor=north},
        documents/.style={rectangle, draw=black!60, fill=black!5, very thick, minimum size=5mm,anchor=north},
    ]
        %Nodes
        \node[ontology] (ontology) at (0,0) {Ontology};

        \node[documents,align=center] (rdf) at (3,-1.5) {RDF documents};

        \node[squarednode,align=center] (schema1) at (-3,-1.5) {Universal\\schema};

        \node[schema,align=center] (xml1) at (-4,-3) {XML\\schema};
        \node[schema,align=center] (json1) at (-2,-3) {JSON\\schema};

        \node[documents,align=center] (xml_document) at (1.5,-3.5) {XML\\document};
        \node[documents,align=center] (json_document) at (4.5,-3.5) {JSON\\document};


        %Lines
        \draw[-latex] (rdf) -- node[above,anchor=south west,align=left] {conforms} (ontology);

        \draw[-latex] (ontology) -- (schema1);
        \draw[-latex] (schema1) -- (xml1);
        \draw[-latex] (schema1) -- (json1);
        \draw[-latex] (xml_document) to[bend left] node[below, anchor=north] {conforms} (xml1);
        \draw[-latex] (json_document) to[bend left] node[below, anchor=north] {conforms} (json1);

        \draw [->,line width=1pt, transform canvas={xshift=-1.25em}] (xml_document) -- (rdf);
        \draw [->,line width=1pt, transform canvas={xshift=-0.5em}] (rdf) -- (xml_document);

        \draw [->,line width=1pt, transform canvas={xshift=0.5em}] (json_document) -- (rdf);
        \draw [->,line width=1pt, transform canvas={xshift=1.25em}] (rdf) -- (json_document);

        \node[rectangle,fill=white, anchor=north] at (3,-2.5) {lifting and lowering};
        %        \node[matrix,anchor=north west,cells={nodes={anchor=west}},inner sep=1ex] at (5,0) {
            %  \draw[-latex,color=red,line width=1.5pt](0,0) -- ++ (0.6,0); & \node{Lifting to RDF};\\
%  \draw[-latex,color=blue,line width=1.5pt](0,0) -- ++ (0.6,0); & \node{Lowering from RDF};\\
%  \draw[-latex,color=purple](0,0) -- ++ (0.6,0); & \node{conforms given schema};\\
% };
    \end{tikzpicture}
    \caption{Example of data transformation. An XML document that conforms to XML schema may be lifted to RDF representation, which conforms to the ontology. The RDF can then be lowered to another format.}
\end{figure}

% Necaskeho pozadavek na OR a hierarchii
% https://github.com/mff-uk/dataspecer/issues/95

\begin{requirement}
    It shall be possible to easily add more specific classes to the universal schema that extends the base class in a way that data, that conforms the resulting schemas, may contain either the base class, or one of its specialization.
\end{requirement}

\begin{showcase}
    We will start directly with an example. Suppose, that the warehouse also distributes foods besides the general goods. Food is of course type of good, but for storing purposes, it may have additional attributes, such as \textit{storing temperature}. When designing a schema that contains goods anywhere


\end{showcase}

This requirement impacts the application on two different levels. First, the universal schema model has to have constructs to represent the required problem and all generators shall understand them and generate a schema that corresponds to the intended result.

Generaly speaking, inheritance is a complex concept, that can be expressed in more simple way. Unfortunatelly, to make this pleasible for a user, we need to represent some things % such as association to class

% Analyza co vsechno se da udelat, co bude privetive a co ne. Rozeberu to na zakladni OR a include.

\section{Future requirements}

Following requirements in this section are analyzed and may have an impact on the final model that will be discussed in the next chapter. But due to its complexity, full implementation and analysis will be kept as authors future work and this thesis cover only the necessity to not introduce a technical debt.

\begin{requirement}
    It shall be possible to perform a top-down evolution of schemas and other documents from an ontology. The evolution shall be automatic, if possible, and shall also transorm the data that conform the given schemas. The application shall also deduce the changes from an ontology that does not support versioning.
\end{requirement}

Schema evolution is a large topic with many theoreticall issues.

% popis evoluce, priklady


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{requirement}
    As there shall be a support for data transformations between different schemas, the data transfomations shall respect various ontology alignments to transform data between different ontologies. Alignments shall be created also during user modification of the ontology, between the modification and the original ontology.
\end{requirement}

\textbf{Alignment} as defined in TODO is a set of relations between entities \textit{usually} from different ontologies. These relations specify the semantic equivalence between the entities and create a mapping that can transform data from one ontology to another.

There are already well-known RDF predicates that can cover basic alignment. For semantically identicall entities, we may use \verb|owl:equivalentClass| or \verb|skos:exactMatch|. More usefull RDF predicate is \verb|rdfs:subClassOf| for more specific classes representing things.

The later one is already included in the previous requirement TODO. Subclasses (i) reuses attributes and associations from its parent class, but also semantically denotes, that (ii) the subclass can also be treated as "the parent class". The second point is an example of a simple ontology alignment.

\begin{showcase}
    % example with address
\end{showcase}
