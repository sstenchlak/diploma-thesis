\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

During the software development process, we may come to a point where splitting a large codebase into smaller, well-defined chunks is necessary to maintain the growth of our software. Using existing systems and connecting them is also a viable approach to building software. In both cases, we end up with many applications and services working together.

This approach reduces complexity and demands on software developers as each part can be maintained, deployed, and tested separately. Each developer team must know only the portion they maintain and the immediate surrounding. The surrounding is then defined by a protocol - the interface specifying the data which flows between the systems.

Those protocols must be created, documented, and maintained, which can be a long, error-prone task. The result of the process is usually a set of data schemas and documentation for developers. Especially the schemas need to be designed carefully to be, if possible, consistent in format and naming.

\bigskip

As an example, consider a company selling and distributing its own goods. The goods are stored in warehouses and then shipped to customers. For the shipping process, the warehouse workers need to know the properties of the items they need to send. Similarly, customers need to know the properties of items they are buying. This scenario is denoted in the following image. The nodes are individual systems in the company that communicate with each other.

\begin{figure}[h]\centering
\begin{tikzpicture}[
    squarednode/.style={rectangle, draw=purple!60, fill=purple!5, very thick, minimum size=5mm},
]
    %Nodes
    \node[squarednode] (db) {Company's goods database};
    \node[squarednode] (warehouse) [below=2cm of db, xshift=.5cm,anchor=north west] {Warehouse application};
    \node[squarednode] (shop) [below=2cm of db, xshift=-.5cm,anchor=north east] {Online shop};

    %Lines
    \draw[-latex] (db) -- node[text width=3cm,align=center,auto,anchor=west,midway]{order info with items to send} (warehouse);
    \draw[-latex] (db) -- node[text width=3.5cm,align=center,auto,anchor=east,midway]{list of available items} (shop);
\end{tikzpicture}
\end{figure}

Our goal is to describe the protocols formally. Which data are sent and where. By doing this without any tool, we may find several obstacles during the process:

\begin{enumerate}
    \item We may need to describe an entity for every schema that uses it. Hence doing the \textbf{same work multiple times}.
    \item There is a high chance of \textbf{inconsistency}. We may name the semantically identical attributes differently, which may confuse software engineers who came in touch with multiple system parts.
    \item We \textbf{lack context} in the whole system where the entities are used.
\end{enumerate}

Regarding point 1, defining a subschema for shared parts of the domain may not be sufficient, as we may need different detail of information. From the example above, the warehouse application may require only the \textit{name} and \textit{weight} of the items, contrary to the online shop where the \textit{price} is also essential.

In addition to these obstacles, it is hard to provide supporting documentation, diagrams, and examples.

Future modifications to the system or changes in user requirements may enforce altering the schemas, which again may require changing the same thing multiple times and may cause inconsistency. Formally, we will denote changing of schemas as an \textbf{evolution}. The process of evolution is complex, as there can already be existing data that conform to the changed schemas. In that scenario, properly implementing a change in user requirement requires modifying affected schemas, documentation, and all the data (in case the data are stored in the given format).

\section*{Ontology}
\addcontentsline{toc}{section}{Ontology}

The example above tends us to create a formal description and naming of all entities, their properties, and relations in the given domain. Entities represent objects from the world, such as \textit{order}, \textit{customer}, or \textit{goods}. Relations then specify, for example, that \textit{the order belongs to a customer} and \textit{consists of goods}. Formally, such description is called an \textbf{ontology}.

With the data on the web \cite{data-on-the-web} trend in the last few years, even ontologies are becoming accessible publicly. Popular formats include RDFS (or RDF Schema), OWL, UFO, schema.org, and Wikidata. For example, schema.org is a proprietary format describing useful data for search engines, such as events, organizations, or places.

Using pre-defined ontologies in a semi-automatic way of defining schemas is beneficiary because a schema designer may focus entirely on the schema structure and not on the domain semantics.

\section*{OFNs and open data}
\addcontentsline{toc}{section}{OFNs and open data}

\textbf{Open data} is defined as data published on the web without any restrictions on use. This means that anyone can use, modify and distribute the data for any reason, including commercial use.

The definition of open data is very loose but can be further specified by a 5-star scheme designed by Sir Tim Berners-Lee. Each star adds a restriction up until the fifth star describing the Linked Open Data.
\begin{itemize}[noitemsep,leftmargin=2cm]
    \item [1 $\bigstar$] Data are published on the web and can be used freely.
    \item [2 $\bigstar$] Data are structured and in a machine-readable format.
    \item [3 $\bigstar$] The format is not proprietary; hence anyone can open them.
    \item [4 $\bigstar$] Data uses RDF and SPARQL standards from W3C.
    \item [5 $\bigstar$] Data are linked to other data creating a network of data.
\end{itemize}

\smallskip

According to Act 106/1999 Coll. of the Czech Republic, data of public institutions shall be published as open data on the Internet in all formats it was created, and if possible, in a machine-readable format. This description corresponds to two stars in the schema above.

The act then specifies \textbf{OFNs}\footnote{\url{https://data.gov.cz/ofn/}} \textit{(open formal norms)} as recommendations for publishing selected categories of data, such as information about \textit{Tourist destinations} and \textit{Sports centers}. The purpose of these documents is to standardize how these data are published, usually by defining JSON and XML schemas along with textual documentation.

Until recently, all those recommendations (OFNs) were created by hand without any tool.

\smallskip

The process of designing those recommendations is comparable to designing schemas for a software system mentioned above - the designer needs to create schemas and documentation for them with examples and images.

\section*{Focus of the work}
\addcontentsline{toc}{section}{Focus of the work}

This thesis will analyze and extend\footnote{The author implemented the basis of the tool as his research project. This thesis, therefore, introduces advanced constructs and only formalizes those already implemented.} a newly created tool, Dataspecer \cite{dataspecer}, and formally define and analyze its internal model, which follows previous research in the XML data modeling area and extends it to support new requirements.

Due to the complexity of the topic, this thesis does not attempt to cover and implement all details, as some of them will be analyzed by future work. The thesis only focuses on the current author's work and formalizes the basics.

Dataspecer is a web tool for effortless creation and management of data specifications, such as XSD, JSON Schema, and CSV Schema, and the creation of supplementary documents. The tool helps the user model schemas by providing relations from the chosen ontology. Schemas are modeled in a graphical interface in the form of the tree, which is then converted to various schemas.

Supplementary documents are automatically generated files from the modeled schemas, such as:

\begin{itemize}
    \item \textbf{documentation} - Human-readable description of entities from the ontology, that are used in the schema as well as description of the schema.
    \item \textbf{data transformations} - Scripts that can convert data from one schema to another, even between different technologies, such as JSON and XML.
    \item \textbf{examples} - Small datasets that can be used instead of the documentation to understand the schema.
    \item \textbf{example application} - Depending on the context of modeled schema, if the schema describes web-accessible data, it is possible to generate a web application that uses those data as a proof that the workflow works.
\end{itemize}

The tool is already in use for creating new schemas for the public sector of the Czech Republic, including the OFNs.

\bigskip

\noindent The rest of the thesis is organized as follows.

\begin{itemize}
    \item The following chapter \ref{chapters:related-work} analyzes related work and introduces the previous works as the common ground this work will follow - precisely the model-driven approach for data modeling and evolution of XML documents.
    \item Chapter \ref{chapters:analysis} briefly analyzes new requirements for the software as many requirements were already studied in the related work. The chapter focuses on support for schemas other than XML and the data on the web \cite{data-on-the-web} approach to ontology and the modeled schemas.
    It also defines the internal model to represent schemas and changes from the related work where it was first introduced. Changes are also analyzed concerning the new requirements.
    \item The next chapter \ref{chapters:implementation} then briefly describes how this model is integrated into the Dataspecer tool.
    \item The last chapter \ref{chapters:future-work} introduces and briefly analyzes topics for future work, such as the change propagation (evolution) in schemas.
\end{itemize}

